<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://hkunlp.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://hkunlp.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-01T02:36:05+00:00</updated><id>https://hkunlp.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Polaris</title><link href="https://hkunlp.github.io/blog/2025/Polaris/" rel="alternate" type="text/html" title="Polaris"/><published>2025-06-20T00:00:00+00:00</published><updated>2025-06-20T00:00:00+00:00</updated><id>https://hkunlp.github.io/blog/2025/Polaris</id><content type="html" xml:base="https://hkunlp.github.io/blog/2025/Polaris/"><![CDATA[<h1 id="polaris-a-post-training-recipe-for-scaling-reinforcement-learning-on-advanced-reasoning-models">POLARIS: A POst-training recipe for scaling reinforcement Learning on Advanced ReasonIng modelS</h1> <p><strong>Team</strong>: Chenxin An *, Zhihui Xie‚Ä†, Xiaonan Li‚Ä†, Lei Li‚Ä†, Jun Zhang, Shansan Gong, Ming Zhong</p> <p>Jingjing Xu *, Xipeng Qiu, Mingxuan Wang, Lingpeng Kong</p> <p>*: Project Leads; ‚Ä†: Significant Contributor</p> <p><strong>Affiliations</strong>: The University of Hong Kong, Bytedance Seed, Fudan University</p> <div style="background-color: #f5f5f5; padding: 1em; border-radius: 8px;"> <p> We are thrilled to unveil our latest breakthroughs, <code>POLARIS-7B-Preview</code> and <code>POLARIS-4B-Preview</code>, which mark a new frontier in open‚Äêrecipe reasoning models developed using academic‚Äêlevel resources. <code>POLARIS-4B-Preview</code> is fine-tuned from <code>Qwen3-4B</code> and <code>POLARIS-7B-Preview</code> is fine-tuned from <code>Deepseek-R1-Distill-Qwen-7B</code>. Our 4B model achieves an impressive <strong>81.2% Pass@1 accuracy on AIME24</strong> and <strong>79.4% Pass@1 accuracy on AIME25</strong>, outperforming state-of-the-art commercial models like <code>Claude-4-Opus</code>, <code>Grok-3-Beta</code>, and <code>o3-mini-high(2025/01/31)</code> via scaling reinforcement learning on open-source data. On AIME25, POLARIS astonishingly achieves comparable performance to <code>Qwen3-235B-A22B</code> while using less than <strong>2%</strong> of its parameters and can be deployed on consumer-grade GPUs. </p> <p> To foster progress in scaling RL on advanced reasoning models, we are open-sourcing our dataset, code, and training details for the research community. </p> <p> üë®‚Äçüíª¬†<a href="https://github.com/ChenxinAn-fdu/POLARIS">Github</a> | ü§ó¬†<a href="https://huggingface.co/POLARIS-Project/Polaris-4B-Preview">HF Model</a> | ü§ó¬†<a href="https://huggingface.co/datasets/POLARIS-Project/Polaris-Dataset-53K">HF Dataset</a> | üìñ <a href="comming soon">paper</a> | üîé¬†<a href="https://github.com/ChenxinAn-fdu/POLARIS/tree/main/evaluation">Evaluation results</a> </p> </div> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image-1400.webp"/> <img src="/assets/img/polaris-imgs/image.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div> <h4>‚úÖ Takeaways for post-training of advanced reasoning models</h4> <div style="background-color: #ffffff; padding: 1em; border-radius: 8px; margin-bottom: 0px;"> <p style="margin: 0;"> <strong>Data Difficulty:</strong> Before training, Polaris analyzes and maps the distribution of data difficulty. The dataset should not be overwhelmed by either overly difficult or trivially easy problems. We recommend using a data distribution with a slight bias toward challenging problems, which typically exhibits a mirrored J-shaped distribution. </p> </div> <div style="background-color: #f5f5f5; padding: 1em; border-radius: 8px; margin-bottom: 0px;"> <p style="margin: 0;"> <strong>Diversity-Based Rollout:</strong> We leverage the <em>diversity among rollouts</em> to initialize the sampling temperature, which is then progressively increased throughout the RL training stages. </p> </div> <div style="background-color: #ffffff; padding: 1em; border-radius: 8px; margin-bottom: 0px;"> <p style="margin: 0;"> <strong>Inference-Time Length:</strong> Polaris incorporates length extrapolation techniques for generating longer CoT at inference stage. This enables a <em>"train-short, generate-long"</em> paradigm for CoT reasoning, mitigating the computational burden of training with excessively long rollouts. </p> </div> <div style="background-color: #f5f5f5; padding: 1em; border-radius: 8px;"> <p style="margin: 0;"> <strong>Exploration Efficiency:</strong> Exploration efficiency in Polaris is enhanced through multi-stage training. However, reducing the model's response length in the first stage poses potential risks. A more conservative approach would be to directly allow the model to "think longer" from the beginning. </p> </div> </div> <hr/> <h1 id="polariss-recipe">POLARIS‚Äôs Recipe</h1> <p>Current work (e.g., <a href="https://www.notion.so/19681902c1468005bed8ca303013a4e2?pvs=21">DeepscaleR</a>) demonstrates that a small model (e.g., 1.5B parameters) can achieve surprising improvements in reasoning tasks through scaling RL training. However, when we apply their recipe to train more advanced reasoning models, we observe marginal improvements even decline during the RL training of <code class="language-plaintext highlighter-rouge">Qwen3</code>. This suggests a critical gap in the open-source community‚Äôs understanding of how to further scale RL on advanced reasoning models. To address this, we introduce <strong>POLARIS</strong>‚Äîa post-training recipe centered on calibrated data difficulty, enhanced data diversity, inference-time length scaling, and efficient training.</p> <p>We are committed to transparency and will be open-sourcing our trained models, training code, and data to foster community progress.</p> <h2 id="1-data-difficulty"><strong><em>1. Data Difficulty</em></strong></h2> <p>Our POLARIS recipe builds upon a deep investigation on the training data difficulty. Specifically, we conduct controlled experiments regarding data difficulty measured by model pass rate, and choose public available training datasets to enable better reproducibility.</p> <h3 id="balanced-data-difficulty-matters">Balanced Data Difficulty Matters</h3> <p>Our initial experiments involve training models of different scales on the public <a href="https://huggingface.co/datasets/agentica-org/DeepScaleR-Preview-Dataset">DeepScaleR dataset</a>. While a 1.5B model shows significant performance gains as expected, a 7B model trained on the same data exhibits only marginal improvements. We observe that the 7B model‚Äôs average reward quickly surpasses 0.7, indicating that the training set is too simple to drive further improvements.</p> <p>This leads us to a core hypothesis: <strong>For effective RL training, the difficulty of the data must be carefully calibrated to the model‚Äôs scale and capability.</strong></p> <p>To validate this, we analyze the difficulty distribution of the 40,000 samples in the DeepScaleR training set. We use <code class="language-plaintext highlighter-rouge">Deepseek-R1-Distill-Qwen-7B</code> and its <code class="language-plaintext highlighter-rouge">1.5B</code> version to perform an offline evaluation, generating 8 solutions for each problem with a sampling temperature of 0.6. The percentage of correct solutions serves as a proxy for the difficulty of each sample.</p> <p>The results, shown in the figure below, are revealing.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%200-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%200-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%200-1400.webp"/> <img src="/assets/img/polaris-imgs/image%200.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Across both model scales, we observe that most problems are either very easy (8/8 correct solutions) or very hard (0/8 correct solutions). Crucially, we note a ‚Äúmirror effect‚Äù between the two models:</p> <ul> <li>The <strong>1.5B model</strong> shows a mirrored J-shaped (·Ç±) distribution, with most problems being extremely difficult (0/8 correct).</li> <li>The <strong>7B model</strong> shows a standard J-shaped distribution, with the vast majority of problems being far too easy (8/8 correct).</li> </ul> <p>This stark contrast confirms that the dataset, while challenging for a 1.5B model, is not sufficiently difficult to train a 7B model effectively. This insight motivates our work in curating a new, more challenging dataset tailored for advanced models.</p> <p>To confirm our hypothesis, we conduct an ablation study on the <a href="https://huggingface.co/datasets/agentica-org/DeepScaleR-Preview-Dataset">DeepScaleR 40K dataset</a> using the <code class="language-plaintext highlighter-rouge">Deepseek-R1-Distill-Qwen-7B</code> model. We create distinct training sets by systematically altering the difficulty distribution:</p> <ol> <li><strong>Full Dataset (40K samples):</strong> The dataset exhibits the original J-shaped distribution, dominated by easy samples (8/8 correct solutions).</li> <li><strong>Removal of Perfect Scores (26K samples):</strong> We remove all samples with 8/8 correct solutions, creating a mirrored J-shaped distribution.</li> <li><strong>Aggressive Filtering (19K samples):</strong> We filter out all samples with a pass rate greater than 4/8, resulting in an distribution that focuses only on the hardest problems.</li> </ol> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%201-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%201-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%201-1400.webp"/> <img src="/assets/img/polaris-imgs/image%201.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 1: Model performance across the three above mentioned conditions</figcaption> </figure> </div> </div> <p>The results, as shown in Figure¬†1, clearly demonstrate that removing the easiest samples leads to consistent performance improvements. In contrast, both the unfiltered dataset (which lacks sufficient challenge) and the aggressively filtered dataset (which is overly saturated with difficult problems) hinder training progress.</p> <p>These findings confirm that optimal RL training requires <strong>a balanced difficulty distribution</strong>‚Äîone that provides enough challenging samples to drive learning while avoiding both trivial problems and overwhelming difficulty.</p> <h3 id="polariss-data-curation-strategy"><strong>POLARIS‚Äôs Data Curation Strategy</strong></h3> <p>Motivated by these findings, the POLARIS recipe curates a <strong>mirrored J-shape difficulty distribution</strong> by filtering high-quality public datasets, including DeepScaleR-40K, and <a href="https://huggingface.co/datasets/inclusionAI/AReaL-boba-Data">AReaL-boba-106k</a>.</p> <p>Our data engineering process is as follows:</p> <ol> <li><strong>Offline Difficulty Estimation:</strong> We use the specific model being trained to generate <code class="language-plaintext highlighter-rouge">8 rollouts</code> for each potential training problem. The pass rate determines the problem‚Äôs difficulty relative to that model.</li> <li><strong>Targeted Filtering:</strong> To create the desired mirrored J-shape distribution, we remove all samples that the model solves perfectly (8/8 correct).</li> <li><strong>Dataset Assembly and Calibration:</strong> <ul> <li>For training <code class="language-plaintext highlighter-rouge">Deepseek-R1-Distill-Qwen-7B</code>, we applied this filtering to DeepScaleR and AReaL, creating a final training set of <strong>53K samples</strong> (26K from DeepScaleR and 27K from AReaL).</li> <li>To train the the <code class="language-plaintext highlighter-rouge">Qwen3-4B</code> model, we performed an additional filtering pass on this 53K set, resulting in a <strong>30K sample</strong> dataset specifically calibrated to its difficulty level.</li> </ul> </li> </ol> <p>This model-specific calibration of data difficulty is a cornerstone of the POLARIS recipe, ensuring that the training process remains challenging and effective for any given model.</p> <h3 id="dynamically-drop-easy-data-during-training"><strong>Dynamically drop easy data during training</strong></h3> <p>As the RL training process progresses, the model‚Äôs capabilities will grow, and the proportion of difficult questions will decrease. Therefore, in addition to initially adjusting the difficulty distribution, we also adjust the training data distribution during the training process. Here is the figure showing the distribution shift of data difficulty during training.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%202-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%202-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%202-1400.webp"/> <img src="/assets/img/polaris-imgs/image%202.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 2: Data Difficulty Distribution Shifts (Left: Before Training, Right: After-Training; Top: Qwen3-4B, Bottom: Deepseek-R1-distill-Qwen-7B)</figcaption> </figure> </div> </div> <p>We observe that the sample difficulty distribution consistently shifts from a mirrored J-shape (·Ç±) to a J-shape. This evolution reinforces our motivation to start with a ·Ç±-shaped distribution, allowing for a smooth transition to the J-shape.</p> <p>Additionally, since data difficulty changes dynamically during training, using the initial dataset throughout is suboptimal. To maintain an ideal difficulty distribution, we introduce <strong>dynamic difficulty distribution updates</strong>: during training with a rollout size of n=8, we update each sample‚Äôs accuracy after reward computation (initial difficulty determined by offline filtering accuracy). <strong>At the end of each training phase, we remove samples with <code class="language-plaintext highlighter-rouge">Accuracy &gt; 0.9</code> to preserve the initial distribution shape and prevent skewing towards a J-shape<em>.</em></strong></p> <p>This dynamic filtering ensures that the model continues to face appropriately challenging samples, preventing the learning signal from degrading due to an overabundance of mastered samples.</p> <h2 id="2-diversity-based-rollout-sampling"><em>2. Diversity-based Rollout Sampling</em></h2> <p><a href="https://arxiv.org/abs/2501.12948">In GRPO training</a>, diversity is an essential factor. GRPO‚Äôs key is to contrast positive and negative trajectories, increasing the probability of positive ones. The diversity of sampled trajectories is crucial, encompassing two aspects:</p> <ol> <li>High diversity encourages the model to generate both positive and negative trajectories in a single rollout, enhancing trajectory contrast.</li> <li>High diversity allows the model to explore a wider range of potential reasoning paths, which helps prevent the model from quickly becoming overconfident in a narrow set of patterns. We also explore the method to increase the sampling diversity. Our approach aims to achieve the best diversity while ensuring performance.</li> </ol> <p>During the rollout phase, the primary hyperparameters affecting diversity are top‚Äëp,top‚Äëk, and the sampling temperature. In previous open‚Äësource projects, the default settings are typically a top‚Äëp value of 1.0 and a top‚Äëkvalue of ‚Äì1, which together yield maximum diversity. Only the sampling temperature remains adjustable. Temperature is usually established either by following the decoding temperature recommended on the official website (e.g., 0.6) or by setting it as a hyperparameter at 1.0. Therefore, in this section, we focus on temperature to analyze how the temperature affects the RL training performance and propose adjusting the temperature during training to match the base model‚Äôs diversity.</p> <p>We start from a probing experiment to explore the relationship between sampling temperature $t$, performance (mean@32), and diversity among rollouts. To quantify the diversity of the sampled trajectories, we use the distinct N-gram metric. This metric evaluates lexical diversity by measuring the ratio of unique N-grams (contiguous sequences of n words) to the total number of N-grams across all generated outputs. In our experiments we set N=4. A score closer to 1.0 indicates higher diversity, meaning the all trajectories contain a wide variety of phrases with little repetition. Conversely, a score closer to 0 indicates low diversity, suggesting the generated outputs are highly similar or repetitive.</p> <p><strong>Diversity vs. Sampling Temperature:</strong> Higher temperatures bring better diversity. To use more diverse trajectories for training, it is recommended to increase the sampling temperature. At the same temperature, there are significant differences in diversity performance across models. For instance, <code class="language-plaintext highlighter-rouge">Qwen3</code> has fewer unique n-grams and a more concentrated output distribution.</p> <div class="row mt-1"> <div class="col-md-8 mx-auto mt-1 mt-md-0"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%203-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%203-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%203-1400.webp"/> <img src="/assets/img/polaris-imgs/image%203.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 3: Rollout diversity with sampling temperature on R1-Distill-Qwen and Qwen3 across different model sizes</figcaption> </figure> </div> </div> <p><strong>Performance vs. Sampling Temperature:</strong> While pursuing diverse trajectories, it is also necessary to ensure the model‚Äôs performance. When we increase the temperature from 0 to higher, all the tested models‚Äô average accuracy exhibits a low-high-low trend. We also notice that each model has significant differences in their zone spans, highlighting that there is no one-size-fits-all temperature setting . The optimal temperature for achieving a desired level of diversity is highly dependent on the specific model being used.</p> <div class="row mt-1"> <div class="col-md-8 mx-auto mt-1 mt-md-0"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%204-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%204-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%204-1400.webp"/> <img src="/assets/img/polaris-imgs/image%204.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 4: Model performance with sampling temperature on R1-Distill-Qwen and Qwen3 across different model sizes</figcaption> </figure> </div> </div> <h3 id="definition-temperature-zone"><strong>Definition</strong>: Temperature Zone</h3> <p>According to the trends, we can get the following sampling temperature zone empirically:</p> <ol> <li><strong>Robust Generation Zone (RGZ)</strong>: RGZ defines the zone where the model‚Äôs performance is both <code class="language-plaintext highlighter-rouge">optimal</code>and <code class="language-plaintext highlighter-rouge">stable</code><strong>,</strong> without significant increases or decreases. The suggested decoding temperature is typically from RGZ.</li> <li><strong>Controlled Exploration Zone(CEZ):</strong> Temperature in CEZ leads to slight performance degradation compared with RGZone, but the degradation level is acceptable because it leads to the increased rollout diversity.</li> <li><strong>Performance Collapse Zone (PCZ)</strong>: In PCZ, the model tends to output noisy tokens and thus the performance will be extremely low.</li> </ol> <div class="row mt-1"> <div class="col-md-8 mx-auto mt-1 mt-md-0"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%205-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%205-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%205-1400.webp"/> <img src="/assets/img/polaris-imgs/image%205.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>For illustration, We show <code class="language-plaintext highlighter-rouge">Qwen3-4B</code>‚Äôs accuracy on AIME24 to demonstrate the differences among these three areas in the following figure. When temperature is 0.6~1.4, the model achieves the optimal and stable performance curve. When temperature is 1.4-1.55, the performance slightly degrades but has better rollout diversity. Then increasing temperature from 1.55 to 1.7 causes significant model performance collapse, which indicates the PCZone starts from temperature=1.55. Noise begins to appear, making it unsuitable for both training and decoding.</p> <h3 id="temperature-initialization-on-controlled-exploration-zone"><strong>Temperature initialization on Controlled Exploration Zone</strong></h3> <p>Our probing experiments reveal that sampling temperature significantly impacts rollout diversity, and its optimal setting varies across base models. The recommended test temperatures are usually from <strong><code class="language-plaintext highlighter-rouge">Robust Generation Zone</code></strong> , which usually result in low diversity. An overly deterministic sampling temperature restricts the model‚Äôs ability to explore better pattern spaces. In Polaris, we propose initializing the sampling temperature based on the model‚Äôs <strong><code class="language-plaintext highlighter-rouge">Controlled Exploration Zone</code></strong> to achieve comparable performance with improved diversity.</p> <p>We recommend using the sampling temperature at the point where model performance begins to decline while maximizing diversity. For Qwen3-4B and <code class="language-plaintext highlighter-rouge">Deepseek-R1-Distill-Qwen-7B</code>, we set the initial sampling temperatures to 1.4 and 0.7, respectively.</p> <p>The comparison of different temperature initialization settings shows that the most common setting, <code class="language-plaintext highlighter-rouge">t= 0.6 / 1.0</code>, causes a decline in model performance, as it is too low to allow the model to explore better trajectories. In contrast, the temperature within the Controlled Exploration Zone demonstrates the best RL performance.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%206-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%206-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%206-1400.webp"/> <img src="/assets/img/polaris-imgs/image%206.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 5: The performance trend of different sampling temperature initialization</figcaption> </figure> </div> </div> <h3 id="dynamic-temperature-reset-across-training-stages"><strong>Dynamic Temperature Reset Across Training Stages</strong></h3> <p>We also find that the model‚Äôs Robust Generation Zone and Controlled Exploration Zone shift during RL training (as shown in the following figure). Since reinforcement learning increases the probability of positive expression patterns, the model‚Äôs entropy tends to decrease and its exploration space becomes narrower, which is manifested by the convergence of N-grams in different trajectories.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none; margin:auto; width:80%;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%207-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%207-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%207-1400.webp"/> <img src="/assets/img/polaris-imgs/image%207.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 6: The RGZ and CEZ shift towards the high-temperature region after 800 steps of RL training</figcaption> </figure> </div> </div> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none; margin:auto; width:100%;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%208-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%208-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%208-1400.webp"/> <img src="/assets/img/polaris-imgs/image%208.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure 7: Model performance and rollout diversity as a function of RL training steps. The experiment uses a training/testing temperature of 0.7 and a sampling size of 32 for testing.</figcaption> </figure> </div> </div> <p>üìå Since the diversity of sample trajectories is critical for RL training, using the same sampling temperature throughout the training process can result in insufficient diversity at the later training stages and limit the potential for performance gains.</p> <p>Therefore, we propose dynamically updating the temperature during RL training. As the model converges on high-quality patterns, we will increase the sampling temperature to encourage further exploration.</p> <p><strong>After each training stage, we will use a higher sampling temperature to maintain the model‚Äôs previous diversity score.</strong></p> <p>Specifically, we test various sampling temperatures and select the one that achieves the desired diversity score. Our experiments suggest setting the temperature interval based on the previous stage‚Äôs entropy decrease. If entropy decreases slightly, we recommend a 0.05 interval for the sampling temperatures. If entropy decreases significantly, we will use a larger interval. We show the sampling temperatures of each stage of Polaris in this Table:</p> <table> <thead> <tr> <th>¬†</th> <th><strong>Stage-1</strong></th> <th><strong>Stage-2</strong></th> <th><strong>Stage-3</strong></th> </tr> </thead> <tbody> <tr> <td><strong><code class="language-plaintext highlighter-rouge">Polaris-7B-Preview</code></strong></td> <td>0.7</td> <td>1.0</td> <td>1.1</td> </tr> <tr> <td><strong><code class="language-plaintext highlighter-rouge">Polaris-4B-Preview</code></strong></td> <td>1.4</td> <td>1.45</td> <td>1.5</td> </tr> </tbody> </table> <p>To verify the effectiveness of temperature increase, we conduct a baseline with the same temperature across the whole training. As we can see, the multi-stage with increased temperature leads to better RL training and further expanding the model‚Äôs thought depth by expanding the response length.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none; margin:auto; width:100%;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%209-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%209-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%209-1400.webp"/> <img src="/assets/img/polaris-imgs/image%209.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="3-inference-time-length-scaling"><em>**3. Inference-Time Length Scaling</em></h2> <h3 id="insufficient-long-context-training-in-rl-stage"><strong>insufficient</strong> long-context training in RL stage</h3> <p>A significant challenge in developing advanced reasoning models is the cost of long-context training. For instance, our model, based on <code class="language-plaintext highlighter-rouge">Qwen3-4B</code>, has a pre-training context length of <strong>32K</strong>. While we increase the maximum training length to 52K during RL, our experiments reveal a critical limitation. The <code class="language-plaintext highlighter-rouge">clip_ratio</code>, which measures the proportion of training samples that reach the maximum sequence length, remained below 10%. This indicates that very few samples are actually trained at the 52K length. During the RL training process, the inference time for rollouts often consumes a significant amount of resources. Within a single batch, if no additional optimizations are made, shorter samples must wait for longer samples to finish decoding, leading to wasted training time and resources. Therefore, it is not efficient to directly use a large training length. We also try some train-short, test-long methods to increase the inference length given limited training budgets.</p> <h3 id="performance-degradation-beyond-pre-training-length">Performance degradation beyond pre-training length</h3> <p>To quantify the effective Chain-of-Thought (CoT) length of <strong><code class="language-plaintext highlighter-rouge">Polaris-4B-Preview</code></strong> , we conduct an analysis using 60 problems from the AIME 2024/25 datasets. There are 32 rollouts for each problem (for a total of 1,920 rollouts) and grouped them by the length of the response:</p> <ul> <li><strong>Short Rollouts Group</strong>: Responses with a length of less than <strong>16K</strong>.</li> <li><strong>Mid-Length Rollouts Group</strong>: Responses with a length between 16K and 32K.</li> <li><strong>Long Rollouts Group:</strong> Responses with a length exceeding the <strong>32K</strong> pre-training limit.</li> </ul> <p>The <em>Accuracy</em> for each group is calculated using the following formula:</p> <p>$\text{Accuracy} = \frac{\text{Number of Correct Rollouts}}{\text{Total Rollouts in Group}}$</p> <p>The results were striking (blue bars). We observe a dramatic performance drop for responses in the <strong>Long Rollouts Group</strong>, which achieved an accuracy of only 26%.</p> <div class="row mt-1"> <div class="col-md-8 mx-auto mt-1 mt-md-0"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%2010-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%2010-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%2010-1400.webp"/> <img src="/assets/img/polaris-imgs/image%2010.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This finding supports our hypothesis: due to the inefficiencies of long-context RL training, the model struggles to generate effective and accurate long CoTs beyond its original pre-training length, even though the RL training length is set to 52K.</p> <h3 id="training-free-length-extrapolation">Training-free Length Extrapolation</h3> <p>To address this, we introduce length extrapolation technique in long reasoning traces generation, which follows the principle of ‚Äútrain shorter, test longer.‚Äù By adjusting the model‚Äôs <a href="https://arxiv.org/abs/2104.09864">Rotary Position Embeddings (RoPE)</a>, this method allows the model to <strong>maintain its performance</strong> on sequences <strong>longer than</strong> those seen during training, effectively compensating for insufficient long-context training.</p> <p>For ease of implementation, we adopt the Yarn method with a scaling factor of <code class="language-plaintext highlighter-rouge">1.5</code>. While Yarn recommends adjusting the attention temperature during extrapolation, we find that this modification‚Äîthough beneficial for long-context retrieval tasks‚Äîis detrimental for generating long reasoning sequences.</p> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="w">  </span><span class="nl">"rope_scaling"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"attn_factor"</span><span class="p">:</span><span class="w"> </span><span class="mf">1.0</span><span class="p">,</span><span class="w">
    </span><span class="nl">"factor"</span><span class="p">:</span><span class="w"> </span><span class="mf">1.5</span><span class="p">,</span><span class="w">
    </span><span class="nl">"rope_type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"yarn"</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span></code></pre></div></div> <div class="row mt-1"> <div class="col-md-8 mx-auto mt-1 mt-md-0"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%2011-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%2011-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%2011-1400.webp"/> <img src="/assets/img/polaris-imgs/image%2011.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>By applying Yarn at inference time‚Äîwith no retraining required‚Äîwe boost the accuracy on responses longer than 32K from <strong>26% to over 50%!</strong></p> <p>This discovery suggests that CoT extrapolation is a powerful tool for the <strong>later stages</strong> of training advanced reasoning models, especially when increasing rollout length becomes unaffordable. We also note that the accuracy improvements are concentrated on the more difficult problems in the dataset.</p> <h3 id="extrapolation-benefits-inference-time-scaling">Extrapolation Benefits Inference-Time Scaling</h3> <p>The graph below illustrates the inference-time scaling capabilities unlocked by Yarn on the AIME 24/25 datasets. The blue line represents <strong><code class="language-plaintext highlighter-rouge">Polaris-4B-Preview</code></strong> with Yarn, while the orange line shows the baseline performance without it.</p> <p>As the chart demonstrates, Polaris-4B-Preview with Yarn (blue line) significantly outperforms its base model, Qwen3-4B, once the context length exceeds 48K. Its performance continues to grow as the length increases toward 96K. In contrast, the model without Yarn (yellow line) shows its performance plateauing after 64K with almost no further gains.</p> <p>This confirms that applying an extrapolation technique like Yarn at inference time unlocks the model‚Äôs potential to scale its reasoning abilities to much longer contexts, overcoming the limitations imposed by practical RL training constraints.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none; margin:auto; width:100%;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%2012-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%2012-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%2012-1400.webp"/> <img src="/assets/img/polaris-imgs/image%2012.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h2 id="4-exploration-efficiency"><em>4. Exploration Efficiency</em></h2> <p>The success of long CoT training hinges on efficient exploration at the frontier of reward sparsity. In Polaris, exploration efficiency is enhanced through multi-stage training, accompanied with techniques that address response-level and sample reward sparsity. Specifically, we found that:</p> <ul> <li>the previously proposed <strong>‚ÄúThink Shorter, then Longer‚Äù</strong> paradigm does not generalize to all reasoning models; directly training with longer responses can often yield better performance;</li> <li>dynamical sampling can be done easy with the proposed <strong>Rollout Rescue Mechanism</strong> and <strong>Intra-Batch Informative Substitution</strong> techniques.</li> </ul> <h3 id="multi-stage-training">Multi-Stage Training</h3> <p>One of the biggest challenges in optimizing long CoT models with RL is the excessively long output, which results in slow training. To improve training efficiency, we incorporate multi-stage training in all our released models. Specifically, we use shorter context windows in earlier stages. Once the model‚Äôs performance converges, we increase the length of the context windows in the next stage.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none; margin:auto; width:100%;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%2013-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%2013-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%2013-1400.webp"/> <img src="/assets/img/polaris-imgs/image%2013.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="is-think-shorter-then-longer-necessary"><strong>Is ‚ÄúThink Shorter, then Longer‚Äù necessary?</strong></h3> <p>While effective, for multi-stage training it is critical to select the appropriate response length at the first stage:</p> <ul> <li>Not all models are both equally token-efficient: We found that training at a small response length works well for <code class="language-plaintext highlighter-rouge">DeepSeek-R1-Distill-Qwen-7B</code> but not for <code class="language-plaintext highlighter-rouge">Qwen3-4B</code>. Specifically, we observe drastic performance drop for <code class="language-plaintext highlighter-rouge">Qwen3-4B</code> even at a response length of 24K and response clip ratio of &lt;15%. Such performance degeneration is irreversible at later stages.</li> </ul> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none; margin:auto; width:100%;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%2014-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%2014-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%2014-1400.webp"/> <img src="/assets/img/polaris-imgs/image%2014.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <ul> <li>It is usually safer to directly allow the model to ‚Äúthink longer‚Äù from the beginning: For <code class="language-plaintext highlighter-rouge">Qwen3-4B</code>, we observed steadily increasing performance with a 40K response length from scratch, in stark contrast with 24K and 24K‚Üí40K schemes.</li> </ul> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none; margin:auto; width:100%;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%2015-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%2015-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%2015-1400.webp"/> <img src="/assets/img/polaris-imgs/image%2015.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><strong>Takeaway</strong>: When computational resources allow, start directly with the maximum decoding length suggested by the official repository.</p> <h3 id="rollout-rescue-mechanism">Rollout Rescue Mechanism</h3> <p>POLARIS uses a small rollout size (8) for cost savings, but this raises the chance of zero-reward batches on hard prompts. To balance positive examples with minimal engineering, we maintain a per-example offline buffer (‚Äúsink‚Äù):</p> <ol> <li>If all 8 rollouts fail (accuracy 0/8) and a correct rollout was observed in earlier epochs, store that response in the sink (evicting the previous one).</li> <li>In later epochs, whenever a new batch yields 0/8 for that example, randomly swap one failed rollout with the buffered response.</li> </ol> <p>This lightweight strategy reduces zero-reward data dramatically and speeds up convergence, without retry loops.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none; margin:auto; width:100%;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/polaris-imgs/image%2016-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/polaris-imgs/image%2016-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/polaris-imgs/image%2016-1400.webp"/> <img src="/assets/img/polaris-imgs/image%2016.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="intra-batch-informative-substitution">Intra-Batch Informative Substitution</h3> <p>In GRPO, examples with all-correct or all-incorrect rollouts produce no advantage. Rather than complex dynamic sampling, we apply a simple in-batch swap:</p> <ol> <li>Within each batch, select samples that have a mix of correct and incorrect rollouts (nonzero advantage).</li> <li>Randomly duplicate these informative samples to replace those that yield zero advantage.</li> </ol> <p>This ensures every training example contributes a learning signal, matching DAPO‚Äôs benefits but requiring only a few tensor index operations‚Äîno extra rollouts or data-pipeline changes.</p> <h2 id="5-from-dapo-and-grpo"><strong><em>5. From DAPO and GRPO+</em></strong></h2> <p>We‚Äôve incorporated several key strategies from <a href="https://dapo-sia.github.io/">DAPO</a> and <a href="https://www.notion.so/1cf81902c14680b3bee5eb349a512a51?pvs=21">GRPO+</a> into our training process for the following reasons:</p> <ul> <li><strong>No Entropy Loss (from GRPO+):</strong> We remove the entropy loss term to prevent training instability. While intend to encourage exploration, we note it can cause entropy to grow uncontrollably, leading to a training collapse. Our primary motivation is to ensure a more stable and reliable training process.</li> <li><strong>No KL Loss (from DAPO):</strong> We eliminate the KL loss to allow our model to explore beyond the constraints of the original SFT model. This also speeds up training, as we no longer need to compute log probabilities for a reference model.</li> <li><strong>Clip High (from DAPO):</strong> We increase the upper clipping bound in the surrogate loss function to encourage more aggressive exploration. This adjustment helps stabilize entropy and has been shown to improve model performance by allowing the policy to take larger, more beneficial update steps.</li> </ul> <h2 id="6-reward-function"><em>6. Reward Function</em></h2> <p>The reward function used in this work is the same as DeepscaleR, we employ an Outcome Reward Model (ORM) which returns:</p> <ul> <li><code class="language-plaintext highlighter-rouge">1</code> - If the LLM‚Äôs answer passes basic LaTeX/Sympy checks.</li> <li><code class="language-plaintext highlighter-rouge">0</code> - If the LLM‚Äôs answer is incorrect or formatted incorrectly (e.g. missing <code class="language-plaintext highlighter-rouge">&lt;think&gt;</code>, <code class="language-plaintext highlighter-rouge">&lt;/think&gt;</code> delimiters).</li> </ul> <h1 id="evaluation"><strong><em>Evaluation</em></strong></h1> <p>Our model needs to use a <strong>higher</strong> <strong>sampling temperature</strong> and <strong>a longer response length</strong> than Qwen3; all other settings are the same. For AIME24 and AIME25, we report the average performance of 32 runs.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">sampling_params</span> <span class="o">=</span> <span class="nc">SamplingParams</span><span class="p">(</span>
        <span class="n">temperature</span><span class="o">=</span><span class="mf">1.4</span><span class="p">,</span>
        <span class="n">top_p</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
        <span class="n">top_k</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
        <span class="n">max_tokens</span><span class="o">=</span><span class="mi">90000</span>
    <span class="p">)</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">example input:</code> <code class="language-plaintext highlighter-rouge">&lt;|im_start|&gt;user\nEvery morning Aya goes for a $9$-kilometer-long walk and stops at a coffee shop afterwards. When she walks at a constant speed of $s$ kilometers per hour, the walk takes her 4 hours, including $t$ minutes spent in the coffee shop. When she walks $s+2$ kilometers per hour, the walk takes her 2 hours and 24 minutes, including $t$ minutes spent in the coffee shop. Suppose Aya walks at $s+\\frac{1}{2}$ kilometers per hour. Find the number of minutes the walk takes her, including the $t$ minutes spent in the coffee shop. Let's think step by step and output the final answer within \\boxed{}.&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n</code></p> <p>The evaluation scripts based on <a href="https://github.com/volcengine/verl">Verl</a> have been released on our GitHub. You can also use your own scripts for testing, but please note: our model‚Äôs output length has been significantly boosted. If your max response length is set too small, the performance may not even reach the level of the original <strong><code class="language-plaintext highlighter-rouge">Qwen3-4B</code></strong> due to the truncation mechanism. Therefore, please ensure the testing length is at least 64K. The graph showing performance changes with response length is available in the <strong>‚ÄúInference-Time Length Scaling‚Äù</strong> section.</p> <table> <thead> <tr> <th><strong>Models</strong></th> <th><strong>AIME24 avg@32</strong></th> <th><strong>AIME25 avg@32</strong></th> <th><strong>Minerva Math avg@4</strong></th> <th><strong>Olympiad Bench avg@4</strong></th> <th><strong>AMC23 avg@8</strong></th> </tr> </thead> <tbody> <tr> <td><strong><code class="language-plaintext highlighter-rouge">Deepseek-R1-Distill-Qwen-7B</code></strong></td> <td>55.0</td> <td>39.7</td> <td>36.7</td> <td>56.8</td> <td>81.9</td> </tr> <tr> <td><strong><code class="language-plaintext highlighter-rouge">AReal-boba-RL-7B</code></strong></td> <td>61.9</td> <td>48.3</td> <td>39.5</td> <td>61.9</td> <td>86.4</td> </tr> <tr> <td><strong><code class="language-plaintext highlighter-rouge">Skywork-OR1-7B-Math</code></strong></td> <td>69.8</td> <td>52.3</td> <td><strong>40.8</strong></td> <td>63.2</td> <td>85.3</td> </tr> <tr> <td><strong><code class="language-plaintext highlighter-rouge">POLARIS-7B-Preview</code></strong></td> <td><strong>72.6</strong></td> <td><strong>52.6</strong></td> <td>40.2</td> <td><strong>65.4</strong></td> <td><strong>89.0</strong></td> </tr> <tr> <td><strong><code class="language-plaintext highlighter-rouge">Deepseek-R1-Distill-Qwen-32B</code></strong></td> <td>72.6</td> <td>54.9</td> <td>42.1</td> <td>59.4</td> <td>84.3</td> </tr> <tr> <td><strong><code class="language-plaintext highlighter-rouge">qwen3-32B</code></strong></td> <td>81.4</td> <td>72.9</td> <td>44.2</td> <td>66.7</td> <td>92.4</td> </tr> <tr> <td><strong><code class="language-plaintext highlighter-rouge">qwen3-4B</code></strong></td> <td>73.8</td> <td>65.6</td> <td>43.6</td> <td>62.2</td> <td>87.2</td> </tr> <tr> <td><strong><code class="language-plaintext highlighter-rouge">POLARIS-4B-Preview</code></strong></td> <td><strong>81.2</strong></td> <td><strong>79.4</strong></td> <td><strong>44.0</strong></td> <td><strong>69.1</strong></td> <td><strong>94.8</strong></td> </tr> </tbody> </table> <h2 id="citation">Citation</h2> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">Polaris2025</span><span class="p">,</span>
    <span class="na">title</span> <span class="p">=</span> <span class="s">{POLARIS: A Post-Training Recipe for Scaling Reinforcement Learning on Advanced Reasoning Models}</span><span class="p">,</span>
    <span class="na">url</span> <span class="p">=</span> <span class="s">{https://hkunlp.github.io/blog/2025/Polaris}</span><span class="p">,</span>
    <span class="na">author</span> <span class="p">=</span> <span class="s">{An, Chenxin and Xie, Zhihui and Li, Xiaonan and Li, Lei and Zhang, Jun and Gong, Shansan and Zhong, Ming and Xu, Jingjing and Qiu, Xipeng and Wang, Mingxuan and Kong, Lingpeng}</span>
    <span class="nv">year</span> <span class="err">=</span> <span class="err">{2025</span><span class="p">}</span>
<span class="c">}</span>
</code></pre></div></div>]]></content><author><name>Chenxin An</name></author><category term="reasoning-models"/><category term="scaling-RL"/><summary type="html"><![CDATA[Introducing Polaris-4B-Preview and Polaris-7B-Preview, the most powerful open-recipe reasoning models to date.]]></summary></entry><entry><title type="html">EvaByte: Efficient Byte-level Language Models at Scale</title><link href="https://hkunlp.github.io/blog/2025/evabyte/" rel="alternate" type="text/html" title="EvaByte: Efficient Byte-level Language Models at Scale"/><published>2025-01-21T00:00:00+00:00</published><updated>2025-01-21T00:00:00+00:00</updated><id>https://hkunlp.github.io/blog/2025/evabyte</id><content type="html" xml:base="https://hkunlp.github.io/blog/2025/evabyte/"><![CDATA[<div style="display:none"> $$ \definecolor{strings}{rgb}{.824,.251,.259} \definecolor{keywords}{rgb}{.224,.451,.686} \definecolor{comment}{rgb}{.322,.451,.322} \newcommand{\norm}[1]{\left\lVert#1\right\rVert} \newcommand{\coloneqq}{\mathrel{\vcenter{:}}=} \newcommand{\R}{\mathbb{R}} \newcommand{\mathbold}[1]{\boldsymbol{\mathbf{#1}}} \newcommand{\mcK}{\mathcal{K}} \newcommand{\mcN}{\mathcal{N}} \newcommand{\mcO}{\mathcal{O}} \newcommand{\mcP}{\mathcal{P}} \newcommand{\mcC}{\mathcal{C}} \newcommand{\mcS}{\mathcal{S}} \newcommand{\mcL}{\mathcal{L}} \newcommand{\mba}{\mathbold{a}} \newcommand{\mbb}{\mathbold{b}} \newcommand{\mbc}{\mathbold{c}} \newcommand{\mbd}{\mathbold{d}} \newcommand{\mbe}{\mathbold{e}} \newcommand{\vf}{\mathbold{f}} \newcommand{\mbg}{\mathbold{g}} \newcommand{\mbh}{\mathbold{h}} \newcommand{\mbi}{\mathbold{i}} \newcommand{\mbj}{\mathbold{j}} \newcommand{\mbk}{\mathbold{k}} \newcommand{\mbl}{\mathbold{l}} \newcommand{\mbm}{\mathbold{m}} \newcommand{\mbn}{\mathbold{n}} \newcommand{\mbo}{\mathbold{o}} \newcommand{\mbp}{\mathbold{p}} \newcommand{\mbq}{\mathbold{q}} \newcommand{\mbr}{\mathbold{r}} \newcommand{\mbs}{\mathbold{s}} \newcommand{\mbt}{\mathbold{t}} \newcommand{\mbu}{\mathbold{u}} \newcommand{\mbv}{\mathbold{v}} \newcommand{\mbw}{\mathbold{w}} \newcommand{\mbx}{\mathbold{x}} \newcommand{\mby}{\mathbold{y}} \newcommand{\mbz}{\mathbold{z}} \newcommand{\mbA}{\mathbold{A}} \newcommand{\mbB}{\mathbold{B}} \newcommand{\mbC}{\mathbold{C}} \newcommand{\mbD}{\mathbold{D}} \newcommand{\mbE}{\mathbold{E}} \newcommand{\mbF}{\mathbold{F}} \newcommand{\mbG}{\mathbold{G}} \newcommand{\mbH}{\mathbold{H}} \newcommand{\mbI}{\mathbold{I}} \newcommand{\mbJ}{\mathbold{J}} \newcommand{\mbK}{\mathbold{K}} \newcommand{\mbL}{\mathbold{L}} \newcommand{\mbM}{\mathbold{M}} \newcommand{\mbN}{\mathbold{N}} \newcommand{\mbO}{\mathbold{O}} \newcommand{\mbP}{\mathbold{P}} \newcommand{\mbQ}{\mathbold{Q}} \newcommand{\mbR}{\mathbold{R}} \newcommand{\mbS}{\mathbold{S}} \newcommand{\mbT}{\mathbold{T}} \newcommand{\mbU}{\mathbold{U}} \newcommand{\mbV}{\mathbold{V}} \newcommand{\mbW}{\mathbold{W}} \newcommand{\mbX}{\mathbold{X}} \newcommand{\mbY}{\mathbold{Y}} \newcommand{\mbZ}{\mathbold{Z}} \newcommand{\mbphi}{\mathbold{\phi}} \newcommand{\mbpsi}{\mathbold{\psi}} \newcommand{\mcM}{\mathcal{M}} \newcommand{\mcK}{\mathcal{K}} \newcommand{\mcN}{\mathcal{N}} \newcommand{\mcO}{\mathcal{O}} \newcommand{\mcP}{\mathcal{P}} \newcommand{\mcC}{\mathcal{C}} \newcommand{\mcS}{\mathcal{S}} $$ </div> <p><strong>Full team:</strong> Lin Zheng, Xueliang Zhao, Guangtao Wang, Chen Wu, David Dong, Angela Wang, Mingran Wang, Yun Du, Haige Bo, Amol Sharma, Bo Li, Kejie Zhang, Changran Hu, Urmish Thakker, and Lingpeng Kong</p> <h2 id="introducing-evabyte">Introducing EvaByte</h2> <p>In a collaborative effort between the University of Hong Kong and SambaNova Systems, we introduce <strong>EvaByte</strong>, a 6.5B state-of-the-art <strong>byte-level language model</strong> featuring an improved architecture and powered by EVA ‚Äì an efficient attention mechanism designed for scalability and performance.</p> <p>Trained on 1.5T bytes of natural language text, math, and code using the performant SambaNova SN30 RDU system, EvaByte demonstrates that efficient byte-level processing at scale is not just possible, but practically advantageous ‚Äì rivaling modern open-source tokenizer-based LMs <d-cite key="groeneveld2024olmo,li2024dclm,zhang2024mapneo"></d-cite> despite using 5x less training data, excelling in coding tasks, and decoding up to 2x faster. Its token-free design also brings added <strong>flexibility</strong>, avoiding tokenizer quirks while naturally extending to <a href="#case-study-multimodal-learning">multimodal applications</a> without any architecture tweaks.</p> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/perf_data_scaling-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/perf_data_scaling-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/perf_data_scaling-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/perf_data_scaling.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: scaling analysis between average task performance and training set size.</figcaption> </figure> </div> </div> <div class="row mt-1"> <div class="col-sm-12 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/main_table_v2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/main_table_v2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/main_table_v2-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/main_table_v2.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: comparison of language models on standard evaluation benchmarks. ‚Ä° the number of tokens measured by Llama 3 tokenizer, corresponding to 1.5T training bytes. ‚Ä†Low scores are caused by failing to generate Python functions and repeat the input under EvalPlus prompt format.</figcaption> </figure> </div> </div> <p>To our knowledge, EvaByte is the first open-source byte-level model without tokenization that yet matches the performance of modern tokenizer-based LMs. Check out the model weights and code here:</p> <ul> <li>Base model before annealing: <a href="https://huggingface.co/EvaByte/EvaByte-Phase1"><strong>EvaByte/EvaByte-Phase1</strong></a></li> <li>Base model: <a href="https://huggingface.co/EvaByte/EvaByte"><strong>EvaByte/EvaByte</strong></a></li> <li>SFT model: <a href="https://huggingface.co/EvaByte/EvaByte-SFT"><strong>EvaByte/EvaByte-SFT</strong></a></li> <li>Codebase: <a href="https://github.com/OpenEvaByte/evabyte"><strong>GitHub</strong></a></li> </ul> <h2 id="byte-level-modeling-with-improved-architectures">Byte-level Modeling with Improved Architectures</h2> <p>Tokenization is a fundamental step in modern large language models, deciding how input is represented in Transformers. Although it efficiently compresses raw text into shorter sequences, tokenization comes with its own baggage ‚Äì it is an externally trained, detached component that can introduce complex biases and edge-case quirks, like the prompt boundary problem <d-cite key="microsoft2023guidance,lundberg2023tokenhealing,dagan2024getting,athiwaratkun2024token,vieira2024language"></d-cite>, undertrained tokens <d-cite key="rumbelow2023solidgoldmagikarp,land2024fishing,wang2024tokenizationmatters,yang2024rethinking,yang2024problematictokens"></d-cite>, and even pretraining data mixture leaks <d-cite key="hayase2024datamixture"></d-cite>.</p> <p>Byte-level modeling is an approach that inherently eliminates biases introduced by tokenization, although directly operating on bytes at scale is not easy <d-cite key="clark2022canine,xue2022byt5,tay2022charformer,yu2023megabyte,slagle2024spacebyte,wang2024mambabyte,kallini2024mrt5"></d-cite>: </p> <div class="row mt-0"> <div class="col-sm-10 mt-1 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/tokens_to_bytes_normalized-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/tokens_to_bytes_normalized-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/tokens_to_bytes_normalized-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/tokens_to_bytes_normalized.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: correspondence between tokens and bytes, as measured by the GPT-4o tokenizer.</figcaption> </figure> </div> </div> <ul> <li>Byte sequences are naturally longer ‚Äì 3.8x longer than their tokenized counterparts in our training corpus ‚Äì leading to more than 3.8x computational overhead under standard Transformer architectures.</li> <li>Inference becomes more challenging due to the inherently long and sequential nature of byte-level predictions.</li> <li>Training byte-level models is less stable as we observed in our <a href="#training">experiments</a>.</li> </ul> <p>We address these hurdles with a streamlined architecture featuring two improvements: <strong>multibyte prediction</strong> and <strong>the efficient attention mechanism, EVA</strong>.</p> <div class="row mt-1"> <div class="col-sm-10 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/arch-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/arch-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/arch-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/arch.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: an overview of the EvaByte architecture.</figcaption> </figure> </div> </div> <p>Although vanilla byte-level language models typically run much slower than tokenizer-based LMs, with the improved architecture, we have achieved a significant speed boost for byte models ‚Äì <strong>5-10x faster</strong> decoding compared to vanilla architectures and even <strong>up to 2x faster</strong> than tokenizer-based LMs, making byte-level models a practical choice for real-world applications.</p> <div class="row mt-1"> <div class="col-sm-10 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/decoding_runtime-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/decoding_runtime-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/decoding_runtime-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/decoding_runtime.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: <b>bytes per second</b> (ü†Ö) measured by generating 512 bytes (or tokens) with a batch size of 1 on one H800 GPU using the HF native generate() interface.</figcaption> </figure> </div> </div> <h3 id="multibyte-prediction">Multibyte Prediction</h3> <p>We draw inspiration from recent work <d-cite key="stern2018blockwise,qi2020prophetnet,cai2024medusa,gloeckle2024multitoken"></d-cite> and equip our model with multiple prediction heads, allowing it to predict several future bytes simultaneously. During training, we average the cross-entropy losses from different output heads as the primary training objective. These heads learn very effectively ‚Äì their predictions are often highly accurate and sometimes even outperform the immediate next byte prediction, as shown in the figure below.</p> <div class="row mt-1"> <div class="col-sm-10 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/olmes_perf_vs_pred_heads-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/olmes_perf_vs_pred_heads-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/olmes_perf_vs_pred_heads-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/olmes_perf_vs_pred_heads.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: multi-choice task performance across different prediction heads. Each head corresponds to using the likelihood from the immediate next byte prediction (Head 1), second-next byte prediction (Head 2), and so forth.</figcaption> </figure> </div> </div> <p>Multibyte prediction adds almost no training overhead, thanks to the particularly small vocabulary size. <d-footnote>Our model uses 8 prediction heads and a vocabulary size of 320, including 256 byte values and 64 special tokens.</d-footnote> However, it greatly speeds up inference with <strong>self-speculative decoding</strong>, where multiple heads are combined via Medusa-like tree attention <d-cite key="cai2024medusa"></d-cite> and enable the model to predict multiple bytes in one decoding step.</p> <h3 id="efficient-attention-with-eva">Efficient Attention with EVA</h3> <p>However, multibyte prediction alone is not enough to speed up the byte-level model: the self-attention mechanism quickly becomes the major bottleneck as the context length grows. To address this, we build our model on <strong>EVA</strong> <d-cite key="zheng2023eva"></d-cite>, an improved version of <strong>linearized attention</strong> <d-cite key="katharopoulos2020transformers_are_rnns,peng2021rfa,choromanski2021rethinking"></d-cite>. Linearized attention approximates exact self-attention by designing feature maps $\phi(\cdot)$ such that</p> <div style="font-size: 0.9em; auto; text-align: center; max-width: 100%;"> \begin{equation} \frac{\sum_{m=1}^n\exp\left(\mbq_{n}^\top \mbk_{m} \right)\mbv_{m}^\top}{\sum_{m'=1}^n \exp\left(\mbq_{n}^\top \mbk_{m'} \right)} \approx \frac{\sum_{m=1}^n \phi(\mbq_n)^\top \phi(\mbk_m)\mbv_{m}^\top}{\sum_{m'=1}^n\phi(\mbq_{n'})^\top \phi(\mbk_{m'})} = \frac{\phi(\mbq_n)^\top \sum_{m=1}^n \phi(\mbk_m)\mbv_{m}^\top}{\phi(\mbq_{n'})^\top \sum_{m'=1}^n\phi(\mbk_{m'})} \notag. \end{equation} </div> <p>By linearizing $\exp(\cdot)$, one can rearrange the order of computation and achieve linear complexity in sequence length. This approach admits the form of a linear RNN, maintaining a global hidden state. With gating mechanisms and decay coefficients <d-cite key="peng2021rfa,qin2024hgrn2,sun2023retnet,yang2024gla"></d-cite>, it also connects to recent state-space models like Mamba and Mamba-2 <d-cite key="gu2024mamba,dao2024mamba2"></d-cite>. Conventional linearized attention compresses past tokens into a single global hidden state, unlike standard attention, which explicitly caches every token.</p> <p>EVA takes a middle ground by <strong>distributing</strong> the global state into multiple local memory slots. By splitting key-value pairs into consecutive chunks and applying linearization <strong>separately</strong> on each chunk, EVA maintains a local hidden state for each chunk and aggregates them together to produce the final output. This expands the design space of linearized attention mechanisms, simplifies implementation, and directly benefits from hardware-optimized kernels for standard attention mechanisms.</p> <div class="row mt-1"> <div class="col-sm-10 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/attn_sketch-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/attn_sketch-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/attn_sketch-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/attn_sketch.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: computation graphs for standard attention (<strong>left</strong>), linearized attention (<strong>middle</strong>), and EVA (<strong>right</strong>). Symbols: $\times$ denotes (multiple) matrix multiplication and $\sum$ represents sum reduction.</figcaption> </figure> </div> </div> <h2 id="training">Training</h2> <p>We pretrain EvaByte on a corpus of 1.5T bytes spanning from text to math and code, mainly sourced from <a href="https://huggingface.co/datasets/allenai/dolma">Dolma v1.7</a>, <a href="https://huggingface.co/datasets/bigcode/the-stack-v2-train-smol-ids">The Stack v2</a>, <a href="https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu">FineWeb-Edu</a>, and <a href="https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0">DCLM-Baseline</a>. We constantly refined the data mix by tweaking the proportions or swapping in new sources mid-flight. After training on 1.2T bytes, we conduct two independent annealing runs (100B and 200B bytes respectively), where the learning rate is linearly decayed from 1e-4 to 0 and the checkpoints are merged via model soup. <d-cite key="wortsman22modelsoups"></d-cite></p> <p>EvaByte is trained with a batch size of 8M bytes and 32K context length on 256 SambaNova SN30-2 RDUs. We observed non-trivial instability during pretraining:</p> <ul> <li><strong>Byte-level collapses</strong>: Occasionally, intermediate checkpoints would produce bizarre typos (e.g., <code class="language-plaintext highlighter-rouge">e</code> in generated outputs turning into an <code class="language-plaintext highlighter-rouge">i</code>) when prompted to perform generation tasks; interestingly, these glitches resolved themselves after a few thousand training steps and never appeared near the end of training.</li> </ul> <figure style="width: 92%; margin: 0 auto;"> <figcaption>A snapshot of code generation at an intermediate checkpoint with bizarre typos.</figcaption> <pre style=" font-size: 12px; font-family:monospace;color: rgb(0, 0, 0); background-color: rgb(254, 252, 252); font-weight: 100; white-space: pre-wrap; word-wrap: break-word; margin: 0; line-height: 1.25; ">
<span style="color: rgb(0, 0, 255); font-weight: 400;">from</span> typing <span style="color: rgb(0, 0, 255); font-weight: 400;">import</span> <span style="color: rgb(163, 21, 21); font-weight: 400;">List</span>, <span style="color: rgb(163, 21, 21); font-weight: 400;">Tuple</span>

<span style="color: rgb(0, 0, 255); font-weight: 400;">def</span> <span style="color: rgb(163, 21, 21); font-weight: 400;">sum_product</span>(<span style="color: rgb(0, 0, 0); font-weight: 400;">numbers: <span style="color: rgb(163, 21, 21); font-weight: 400;">List</span>[<span style="color: rgb(0, 0, 255); font-weight: 400;">int</span>]</span>) -&gt; <span style="color: rgb(163, 21, 21); font-weight: 400;">Tuple</span>[<span style="color: rgb(0, 0, 255); font-weight: 400;">int</span>, <span style="color: rgb(0, 0, 255); font-weight: 400;">int</span>]:
    <span style="color: rgb(163, 21, 21); font-weight: 400;">""" For a given list of integers, return a tuple consisting of a sum and a product of all the integers in a list.
    Empty sum should be equal to 0 and empty product should be equal to 1.
    &gt;&gt;&gt; sum_product([])
    (0, 1)
    &gt;&gt;&gt; sum_product([1, 2, 3, 4])
    (10, 24)
    """</span>
    <span style="color: rgb(0, 0, 255); font-weight: 400;">sum</span> = <span style="color: rgb(0, 0, 0); font-weight: 400;">0</span>
    product = <span style="color: rgb(0, 0, 0); font-weight: 400;">1</span>
    <span style="color: rgb(0, 0, 255); font-weight: 400;">for</span> number <span style="color: rgb(0, 0, 255); font-weight: 400;">in</span> numb<span style="background-color: #ffb6c1;">i</span>rs:
        <span style="color: rgb(0, 0, 255); font-weight: 400;">sum</span> += numb<span style="background-color: #ffb6c1;">i</span>r
        product *= numb<span style="background-color: #ffb6c1;">i</span>r
    <span style="color: rgb(0, 0, 255); font-weight: 400;">return</span> (<span style="color: rgb(0, 0, 255); font-weight: 400;">sum</span>, product)
</pre> </figure> <ul> <li><strong>Loss spikes</strong>: The most helpful techniques for stabilizing training through our experiments include <ul> <li>Lowering Adam epsilon $\epsilon$ from 1e-8 to 1e-12.</li> <li>Skipping batches that lead to spikes to keep the model in sane state.</li> <li>Periodically resetting Adam optimizer states to zero with quickly re-warming up the learning rate to remove bad out-of-track estimates.</li> </ul> <p>Other attempts, like freezing embedding parameters or applying weighted average over different prediction heads, offered little improvement.</p> </li> </ul> <h2 id="empirical-results">Empirical Results</h2> <p>Let‚Äôs dive into how EvaByte performs in practice. We compare EvaByte‚Äôs intermediate checkpoints against recent language models (OLMo-1.7-7B and OLMo-2-7B), trained on the roughly same amount of data. We observe the EvaByte checkpoint at 1.22T bytes (roughly 0.4T tokens) consistently outperforms them by a large margin.</p> <div class="row mt-1"> <div class="col-sm-10 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/intermediate_ckpts_evabyte_vs_olmo-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/intermediate_ckpts_evabyte_vs_olmo-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/intermediate_ckpts_evabyte_vs_olmo-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/intermediate_ckpts_evabyte_vs_olmo.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: performance of intermediate checkpoints on standard benchmarks.</figcaption> </figure> </div> </div> <p></p> <p>We also tracked EvaByte‚Äôs task performance throughout pretraining and observed a consistent <strong>upward trend with no signs of plateauing</strong>. Interestingly, EvaByte excels at coding tasks (e.g., HumanEval and MBPP), even though we intentionally reduced the proportion of code data in the later stages of training. One possible reason is that removing tokenization might eliminate domain-specific biases, enabling more efficient parallel learning across domains. A deeper investigation into this behavior is planned for future work.</p> <div class="row mt-1"> <div class="col-sm-10 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/perf_vs_pretrain_iters-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/perf_vs_pretrain_iters-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/perf_vs_pretrain_iters-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/perf_vs_pretrain_iters.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <h3 id="supervised-fine-tuning">Supervised Fine-tuning</h3> <p>We take EvaByte a step further with <strong>supervised fine-tuning</strong>. Following DCLM <d-cite key="li2024dclm"></d-cite>, OLMo-2 <d-cite key="ai22024olmo2"></d-cite>, TULU 3 <d-cite key="lambert2024tulu3"></d-cite>, and OpenCoder <d-cite key="huang2024opencoder"></d-cite>, we curate a data mix from Tulu 3, OpenHermes 2.5, and OpenCoder, fine-tune EvaByte for 2 epochs, and achieve results on par with recent open LMs.</p> <div class="row mt-1"> <div class="col-sm-10 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/sft_table-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/sft_table-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/sft_table-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/sft_table.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Figure: performance of instruct models. ‚Ä† Evaluated by us. * Following Tulu 3, we evaluate the Pass@10 rate for HumanEval with 20 samples at temperature 0.8.</figcaption> </figure> </div> </div> <h3 id="flexibility">Flexibility</h3> <p>As mentioned at the beginning, we demonstrate below that byte-level modeling naturally avoids tokenization quirks and edge-case behaviors, such as the <strong>prompt boundary problem</strong>, where tokenizer-based LMs behave inconsistently around prompt boundaries. EvaByte resolves these cases seamlessly and delivers more predictable results.</p> <figure> <figcaption style="font-size: 100%; font-family: monospace; padding-bottom: 10px;"> <span style="background-color: #a8dcfb; padding: 0 4px;">&nbsp;&nbsp;&nbsp;&nbsp;</span> prompt &nbsp;&nbsp;&nbsp; <span style="background-color: #B2FBA8; padding: 0 4px;">&nbsp;&nbsp;&nbsp;&nbsp;</span> correct completion &nbsp;&nbsp;&nbsp; <span style="background-color: #ffb6c1; padding: 0 4px;">&nbsp;&nbsp;&nbsp;&nbsp;</span> incorrect completion </figcaption> <figcaption style="display: flex; justify-content: space-between; font-size: 90%; font-family: monospace;"> <span> <strong>EvaByte</strong>: outputs from different prompt boundaries converge. </span> </figcaption> <pre style=" font-size: 0.65em; max-width: 100%; white-space: pre-wrap; word-wrap: break-word; overflow-x: auto; background-color: #f8f9fa; padding: 5px; border-radius: 5px; border: 1px solid #ddd; line-height: 1.25; ">
<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    "</span><span style="background-color: #B2FBA8;">""\n    if not strings:\n        return None\n    longest = strings...</span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    ""</span><span style="background-color: #B2FBA8;">"\n    if not strings:\n        return None\n    longest = strings[...</span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """</span><span style="background-color: #B2FBA8;">\n    if not strings:\n        return None\n    longest = strings[0...</span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n</span><span style="background-color: #B2FBA8;">    if not strings:\n        return None\n    longest = strings[0]...</span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n </span><span style="background-color: #B2FBA8;">   if not strings:\n        return None\n    longest = strings[0]\n...</span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n  </span><span style="background-color: #B2FBA8;">  if not strings:\n        return None\n    longest = strings[0]\n ...</span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n   </span><span style="background-color: #B2FBA8;"> if not strings:\n        return None\n    longest = strings[0]\n  ...</span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n    </span><span style="background-color: #B2FBA8;">if not strings:\n        return None\n    longest = strings[0]\n   ...</span>
</pre> <figcaption style="display: flex; justify-content: space-between; font-size: 90%; font-family: monospace;"> <span> <strong>Qwen2.5-7B</strong>: different prompt boundaries lead to diverging and unexpected outputs. </span> </figcaption> <pre style=" font-size: 0.65em; max-width: 100%; white-space: pre-wrap; word-wrap: break-word; overflow-x: auto; background-color: #f8f9fa; padding: 5px; border-radius: 5px; border: 1px solid #ddd; line-height: 1.25; ">
<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    "</span><span style="background-color: #ffb6c1;">&gt;&gt;&gt; longest([\'a\', \'bb\', \'ccc\', \'dddd\'])\n    \'dddd\'\n    """\n    i...</span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    ""</span><span style="background-color: #ffb6c1;">"""\n    if not strings:\n        return None\n    longest_string =...</span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """</span><span style="background-color: #ffb6c1;"></span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n</span><span style="background-color: #B2FBA8;">    if not strings:\n        return None\n    longest = strings[0]...</span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n </span><span style="background-color: #ffb6c1;"> if not strings:\n    return None\n  longest = strings[0]\n  for st...</span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n  </span><span style="background-color: #ffb6c1;"> # if not strings:\n    #    return None\n    # longest = strings[...</span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n   </span><span style="background-color: #B2FBA8;"> if not strings:\n        return None\n    longest_string = string...</span>

<span style="background-color: #a8dcfb;">‚ñ∂ def longest(strings: List[str]) -&gt; Optional[str]:\n    """ Out of list of strings, return the longest one. ...&gt;&gt;&gt; longest(['a', 'bb', 'ccc'])\n    'ccc'\n    """\n    </span><span style="background-color: #ffb6c1;"> if len(strings) == 0 None:\n        return None\n    else:\n      ...</span>
</pre> </figure> <h2 id="case-study-multimodal-learning">Case Study: Multimodal Learning</h2> <p>EvaByte is also flexible to extend to multimodal tasks, treating image data as just another byte stream according to some protocol, such as JPEG, PNG, etc. We follow prior work <d-cite key="perez2024compressed,han2024jpeglm"></d-cite> and use JPEG format, keeping image byte sequence length manageable (a 384 x 384 image takes around 10K JPEG bytes). Thanks to byte-level modeling, EvaByte can seamlessly interleave image with text bytes for vision-language training <strong>without any architectural tweaks</strong>.</p> <p>Our multimodal dataset spans around 2T bytes, primarily filtered from OBELICS <d-cite key="laurencon2023obelics"></d-cite>, LLaVA-ReCap <d-cite key="li2024llavanext-ablations"></d-cite>, and DataComp <d-cite key="gadre2023datacomp,mobileclip2024"></d-cite>. After 20K fine-tuning steps on the data mix (~3M images), starting from a pre-trained text-only checkpoint, EvaByte can zero-shot generate captions and answer basic visual questions. <d-footnote> EvaByte can also generate images though a bit fuzzily. </d-footnote> The multimodal capabilities of EvaByte are actively evolving, with current work addressing specific alignment challenges (such as spatial relationships between computer components). Scaling and refining these multimodal abilities represents one of our most promising research directions.</p> <table style="width: 100%; border-collapse: collapse;"> <caption style="font-size: 0.9em; color: gray;">Table: after fine-tuning on limited multimodal data, EvaByte demonstrates basic capabilities in image captioning and visual question answering.</caption> <thead> <tr> <th style="text-align: center; width: 30%;">Image</th> <th style="text-align: center; width: 70%;">Text</th> </tr> </thead> <tbody> <tr> <td> <div style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/caption_example_0-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/caption_example_0-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/caption_example_0-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/caption_example_0.jpg" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </td> <td><strong>Detailed caption:</strong> The image shows a close-up view of a laptop keyboard. The keyboard has a black color scheme with white lettering on the keys. The keys are arranged in a standard QWERTY layout, which is commonly used for English language keyboards. The keys are labeled with the standard Western letters and symbols, including the alphabet, numbers, and punctuation marks. The laptop appears to be a modern model, possibly a MacBook, given the design and the visible trackpad to the right of the keyboard. The trackpad is a flat, rectangular surface with a grid pattern, typical of MacBook trackpads. The laptop is open, and the keyboard is the main focus of the image. The background is blurred, but it seems to be an indoor setting with a wooden surface, possibly a table or desk. The lighting in the image is soft, suggesting an indoor environment with ambient lighting.</td> </tr> <tr> <td> <div style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/caption_example_1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/caption_example_1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/caption_example_1-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/caption_example_1.jpg" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </td> <td><strong>Detailed caption:</strong> The image shows a black dog swimming in a body of water. The dog appears to be a medium to large breed, with a glossy black coat. It is captured mid-swim, with its head above the water and its body partially submerged. The water around the dog is rippling, indicating movement, and the surface of the water reflects the light, suggesting it is a sunny day. There are no visible texts or distinguishing marks that provide additional context about the location or the dog's identity. The style of the image is a candid, real-life photograph, capturing a moment of the dog's activity.</td> </tr> <tr> <td> <div style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/vqa_example_0-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/vqa_example_0-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/vqa_example_0-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/vqa_example_0.jpg" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </td> <td> <strong>Q:</strong> How many throw pillows are on the bed?<br/> <strong>A:</strong> three </td> </tr> <tr> <td> <div style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/vqa_example_1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/vqa_example_1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/vqa_example_1-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/vqa_example_1.jpg" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </td> <td> <strong>Q:</strong> Which iconic landmark is on the picture?<br/> <strong>A:</strong> The Eiffel Tower </td> </tr> <tr> <td> <div style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/vqa_example_2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/vqa_example_2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/vqa_example_2-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/vqa_example_2.jpg" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </td> <td> <strong>Q:</strong> What 2 colors are the flowers?<br/> <strong>A:</strong> red and yellow </td> </tr> </tbody> </table> <h2 id="comparison-to-byte-latent-transformers-blts">Comparison to Byte Latent Transformers (BLTs)</h2> <p>A recent concurrent work, Byte Latent Transformers (BLTs) <d-cite key="pagnoni2024blt"></d-cite>, also explores tokenization-free language models and offers an in-depth analysis of BLTs‚Äô behavior at scale. BLTs introduce an elegant framework that first encodes byte sequences into patches and then processes them globally.</p> <p>The main difference between BLTs and EvaByte lies in the <strong>architecture</strong>: BLTs use patchification and propose entropy patching to dynamically group bytes. While this approach adjusts compute allocation based on data complexity and reduces context length, it still relies on external models to determine patch boundaries. The majority of compute ends up focused on patch-level modeling, detached from the byte stream, similar to tokenizer-based models.</p> <p>In contrast, <strong>EvaByte keeps things simple</strong>: it directly operates on bytes with a flat Transformer-like model without needing to invoke external modules or group inputs. Empirically, EvaByte achieves better performance than BLTs even with 3-4x fewer training bytes, as shown in the table below. Besides, EvaByte is more flexible and scales easily to multimodal data, while BLTs require retraining or swapping out the auxiliary language model used for entropy patching.</p> <div class="row mt-1"> <div class="col-sm-11 mt-3 mt-md-0" style="float:none;margin:auto;"> <figure style="margin-top: 0em"> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2025-01-21-evabyte-imgs/comp_to_blt-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2025-01-21-evabyte-imgs/comp_to_blt-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2025-01-21-evabyte-imgs/comp_to_blt-1400.webp"/> <img src="/assets/img/2025-01-21-evabyte-imgs/comp_to_blt.png" class="img-fluid rounded z-depth-0" width="auto" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture><figcaption class="caption">Table: we closely follow the evaluation setup in BLTs, testing zero-shot task performance on Arc-e, Arc-c, HellaSwag, PIQA, and HumanEval; 3-shot for the original MBPP split; and 5-shot for MMLU.</figcaption> </figure> </div> </div> <h2 id="conclusion">Conclusion</h2> <p>We introduce EvaByte, a new family of efficient, scalable, and flexible byte-level language models. The ability to rival tokenization-based LMs with 5x less data while being faster highlights the significant potential of lower-level language modeling within the EvaByte architecture. Future research directions include further refining the model‚Äôs architecture to improve both its capacity and efficiency, analyzing in depth how lower-level language models scale with increasing sizes and data volume, as well as extending the context length to seamlessly process diverse data types ‚Äì images, videos, and audio ‚Äì simultaneously.</p> <h2 id="citation">Citation</h2> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">evabyte</span><span class="p">,</span>
    <span class="na">title</span> <span class="p">=</span> <span class="s">{EvaByte: Efficient Byte-level Language Models at Scale}</span><span class="p">,</span>
    <span class="na">url</span> <span class="p">=</span> <span class="s">{https://hkunlp.github.io/blog/2025/evabyte}</span><span class="p">,</span>
    <span class="na">author</span> <span class="p">=</span> <span class="s">{Lin Zheng and Xueliang Zhao and Guangtao Wang and Chen Wu and David Dong and Angela Wang and Mingran Wang and Yun Du and Haige Bo and Amol Sharma and Bo Li and Kejie Zhang and Changran Hu and Urmish Thakker and Lingpeng Kong}</span><span class="p">,</span>
    <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name>Lin Zheng</name></author><category term="language-models"/><category term="efficient-attention"/><summary type="html"><![CDATA[Introducing EvaByte, an efficient and strong byte-level language model]]></summary></entry></feed>