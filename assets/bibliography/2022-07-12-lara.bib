@inproceedings{peng2021rfa,
title={Random Feature Attention},
author={Hao Peng and Nikolaos Pappas and Dani Yogatama and Roy Schwartz and Noah Smith and Lingpeng Kong},
booktitle={International Conference on Learning Representations},
year={2021},
}
@inproceedings{
choromanski2021rethinking,
title={Rethinking Attention with Performers},
author={Krzysztof Marcin Choromanski and Valerii Likhosherstov and David Dohan and Xingyou Song and Andreea Gane and Tamas Sarlos and Peter Hawkins and Jared Quincy Davis and Afroz Mohiuddin and Lukasz Kaiser and David Benjamin Belanger and Lucy J Colwell and Adrian Weller},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=Ua6zuk0WRH}
}
@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}
@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}
@InProceedings{lara,
  title = 	 {Linear Complexity Randomized Self-attention Mechanism},
  author =       {Zheng, Lin and Wang, Chong and Kong, Lingpeng},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {27011--27041},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/zheng22b/zheng22b.pdf},
  url = 	 {https://proceedings.mlr.press/v162/zheng22b.html},
}
@inproceedings{random-features,
 author = {Rahimi, Ali and Recht, Benjamin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Random Features for Large-Scale Kernel Machines},
 url = {https://proceedings.neurips.cc/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf},
 volume = {20},
 year = {2008}
}
@article{likhosherstov2022chefs,
  title={Chefs' Random Tables: Non-Trigonometric Random Features},
  author={Likhosherstov, Valerii and Choromanski, Krzysztof and Dubey, Avinava and Liu, Frederick and Sarlos, Tamas and Weller, Adrian},
  journal={arXiv preprint arXiv:2205.15317},
  year={2022}
}
@inproceedings{
choromanski2022hybrid,
title={Hybrid Random Features},
author={Krzysztof Marcin Choromanski and Han Lin and Haoxian Chen and Arijit Sehanobish and Yuanzhe Ma and Deepali Jain and Jake Varley and Andy Zeng and Michael S Ryoo and Valerii Likhosherstov and Dmitry Kalashnikov and Vikas Sindhwani and Adrian Weller},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=EMigfE6ZeS}
}
@article{chowdhury2021learning,
  title={On learning the transformer kernel},
  author={Chowdhury, Sankalan Pal and Solomou, Adamos and Dubey, Avinava and Sachan, Mrinmaya},
  journal={arXiv preprint arXiv:2110.08323},
  year={2021}
}
@inproceedings{
irie2021going,
title={Going Beyond Linear Transformers with Recurrent Fast Weight Programmers},
author={Kazuki Irie and Imanol Schlag and R{\'o}bert Csord{\'a}s and J{\"u}rgen Schmidhuber},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=ot2ORiBqTa1}
}
@InProceedings{pmlr-v139-schlag21a,
  title = 	 {Linear Transformers Are Secretly Fast Weight Programmers},
  author =       {Schlag, Imanol and Irie, Kazuki and Schmidhuber, J{\"u}rgen},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {9355--9366},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/schlag21a/schlag21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/schlag21a.html},
}

@InProceedings{ripple,
  title = 	 {Ripple Attention for Visual Perception with Sub-quadratic Complexity},
  author =       {Zheng, Lin and Pan, Huijie and Kong, Lingpeng},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {26993--27010},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/zheng22a/zheng22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/zheng22a.html},
  abstract = 	 {Transformer architectures are now central to sequence modeling tasks. At its heart is the attention mechanism, which enables effective modeling of long-term dependencies in a sequence. Recently, transformers have been successfully applied in the computer vision domain, where 2D images are first segmented into patches and then treated as 1D sequences. Such linearization, however, impairs the notion of spatial locality in images, which bears important visual clues. To bridge the gap, we propose <em>ripple attention</em>, a sub-quadratic attention mechanism for vision transformers. Built upon the recent kernel-based efficient attention mechanisms, we design a novel dynamic programming algorithm that weights contributions of different tokens to a query with respect to their relative spatial distances in the 2D space in linear observed time. Extensive experiments and analyses demonstrate the effectiveness of ripple attention on various visual tasks.}
}
@inproceedings{
zhen2022cosformer,
title={cosFormer: Rethinking Softmax In Attention},
author={Zhen Qin and Weixuan Sun and Hui Deng and Dongxu Li and Yunshen Wei and Baohong Lv and Junjie Yan and Lingpeng Kong and Yiran Zhong},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=Bl8CQrx2Up4}
}

@InProceedings{spe,
  title = 	 {Relative Positional Encoding for Transformers with Linear Complexity},
  author =       {Liutkus, Antoine and C\'{\i}fka, Ond{\v{r}}ej and Wu, Shih-Lun and Simsekli, Umut and Yang, Yi-Hsuan and Richard, Gael},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {7067--7079},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/liutkus21a/liutkus21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/liutkus21a.html},
  abstract = 	 {Recent advances in Transformer models allow for unprecedented sequence lengths, due to linear space and time complexity. In the meantime, relative positional encoding (RPE) was proposed as beneficial for classical Transformers and consists in exploiting lags instead of absolute positions for inference. Still, RPE is not available for the recent linear-variants of the Transformer, because it requires the explicit computation of the attention matrix, which is precisely what is avoided by such methods. In this paper, we bridge this gap and present Stochastic Positional Encoding as a way to generate PE that can be used as a replacement to the classical additive (sinusoidal) PE and provably behaves like RPE. The main theoretical contribution is to make a connection between positional encoding and cross-covariance structures of correlated Gaussian processes. We illustrate the performance of our approach on the Long-Range Arena benchmark and on music generation.}
}
@inproceedings{
luo2021stable,
title={Stable, Fast and Accurate: Kernelized Attention with Relative Positional Encoding},
author={Shengjie Luo and Shanda Li and Tianle Cai and Di He and Dinglan Peng and Shuxin Zheng and Guolin Ke and Liwei Wang and Tie-Yan Liu},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=X7XNPor93uG}
}